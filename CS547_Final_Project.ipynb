{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb2453df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers as ppb # pytorch transformers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as Func\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import random\n",
    "from ipyleaflet import Map, Marker\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b8e662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "# check device available\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fc325ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and divide into training, validation, and testing dataset\n",
    "file = pd.read_csv('https://raw.githubusercontent.com/khordoo/disaster-watch-classifier/master/datasets/combined.csv')\n",
    "tweet = []\n",
    "label = []\n",
    "cat = []\n",
    "\n",
    "for i in range(len(file)):\n",
    "    tweet.append(file.iloc[i][2])\n",
    "    label.append(file.iloc[i][3])\n",
    "    cat.append(file.iloc[i][4])\n",
    "    \n",
    "labels = np.zeros(len(file))\n",
    "d_type = np.zeros(len(file))\n",
    "l = []\n",
    "for i in range(len(file)):\n",
    "    if label[i] == 'on-topic':\n",
    "        labels[i] = 1\n",
    "    else:\n",
    "        labels[i] = 0\n",
    "    if cat[i] in l:\n",
    "        d_type[i] = l.index(cat[i])\n",
    "    else:\n",
    "        l.append(cat[i])\n",
    "        d_type[i] = l.index(cat[i])\n",
    "r = random.sample(range(len(tweet)), len(tweet))\n",
    "tweet_tr = []\n",
    "label_tr = []\n",
    "cat_tr = []\n",
    "tweet_vad = []\n",
    "label_vad = []\n",
    "cat_vad = []\n",
    "tweet_te = []\n",
    "label_te = []\n",
    "cat_te = []\n",
    "for i in range(len(r)):\n",
    "    if i < 500:\n",
    "        tweet_vad.append(tweet[r[i]].lower())\n",
    "        label_vad.append(labels[r[i]])\n",
    "        cat_vad.append(d_type[r[i]])\n",
    "    elif i >= 500 and i < round(len(r)*0.17):\n",
    "        tweet_te.append(tweet[r[i]].lower())\n",
    "        label_te.append(labels[r[i]])\n",
    "        cat_te.append(d_type[r[i]])\n",
    "    else:\n",
    "        tweet_tr.append(tweet[r[i]].lower())\n",
    "        label_tr.append(labels[r[i]])\n",
    "        cat_tr.append(d_type[r[i]])\n",
    "\n",
    "\n",
    "label_tr = np.array(label_tr)\n",
    "cat_tr = np.array(cat_tr)\n",
    "label_vad = np.array(label_vad)\n",
    "cat_vad = np.array(cat_vad)\n",
    "label_te = np.array(label_te)\n",
    "cat_te = np.array(cat_te)\n",
    "label_tr = label_tr.reshape(len(label_tr),1)\n",
    "cat_tr_0 = np.zeros(len(cat_tr))\n",
    "cat_tr_1 = np.zeros(len(cat_tr))\n",
    "cat_tr_2 = np.zeros(len(cat_tr))\n",
    "cat_tr_3 = np.zeros(len(cat_tr))\n",
    "cat_tr_4 = np.zeros(len(cat_tr))\n",
    "cat_tr_5 = np.zeros(len(cat_tr))\n",
    "for i in range(len(cat_tr)):\n",
    "    if cat_tr[i] == 0:\n",
    "        cat_tr_0[i] = 1  \n",
    "    elif cat_tr[i] == 1:\n",
    "        cat_tr_1[i] = 1  \n",
    "    elif cat_tr[i] == 2:\n",
    "        cat_tr_2[i] = 1\n",
    "    elif cat_tr[i] == 3:\n",
    "        cat_tr_3[i] = 1\n",
    "    elif cat_tr[i] == 4:\n",
    "        cat_tr_4[i] = 1\n",
    "    else:\n",
    "        cat_tr_5[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee9e8db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and initilize the model, optimizer\n",
    "\n",
    "class BERT_CUS(nn.Module):\n",
    "    def __init__ (self):\n",
    "        super(BERT_CUS, self).__init__()\n",
    "        self.l0 = BertModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
    "        \n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            #nn.BatchNorm1d(768),\n",
    "            nn.Linear(768, 6),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.l2 = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            #nn.BatchNorm1d(768),\n",
    "            nn.Linear(768, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "            _, output_0= self.l0(input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids)\n",
    "            output1 = self.l1(output_0)\n",
    "            output2 = self.l2(output_0)\n",
    "            return output1, output2\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', use_fast=False, max_length=191)\n",
    "model = BERT_CUS()  \n",
    "for param in model.l0.parameters():\n",
    "    param.requires_grad = True\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54d1a91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "cat acc during vad: 0.7700000000000006\n",
      "label acc during vad: 0.9120000000000007\n",
      "1\n",
      "cat acc during vad: 0.8000000000000006\n",
      "label acc during vad: 0.9220000000000007\n",
      "2\n",
      "cat acc during vad: 0.7740000000000006\n",
      "label acc during vad: 0.9240000000000007\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "B = 5\n",
    "EPOCH = 3\n",
    "L = 3\n",
    "cat_loss = nn.CrossEntropyLoss()\n",
    "label_loss = nn.BCELoss()\n",
    "\n",
    "for e in range(EPOCH):\n",
    "    for i in range(0, len(tweet_tr), B):\n",
    "        optimizer.zero_grad()\n",
    "        encoded = tokenizer(tweet_tr[i:i+B], return_tensors='pt', padding= True)\n",
    "        input_ids = encoded['input_ids'].to(device)\n",
    "        attention_mask = encoded['attention_mask'].to(device)\n",
    "        token_type_ids = encoded['token_type_ids'].to(device)\n",
    "        cat_pre,label_pre = model(input_ids, attention_mask, token_type_ids)\n",
    "        l0 = label_loss(cat_pre[:,0], torch.tensor(cat_tr_0[i:i+B]).float().to(device))\n",
    "        l1 = label_loss(cat_pre[:,1], torch.tensor(cat_tr_1[i:i+B]).float().to(device))\n",
    "        l2 = label_loss(cat_pre[:,2], torch.tensor(cat_tr_2[i:i+B]).float().to(device))\n",
    "        l3 = label_loss(cat_pre[:,3], torch.tensor(cat_tr_3[i:i+B]).float().to(device))\n",
    "        l4 = label_loss(cat_pre[:,4], torch.tensor(cat_tr_4[i:i+B]).float().to(device))\n",
    "        l5 = label_loss(cat_pre[:,5], torch.tensor(cat_tr_5[i:i+B]).float().to(device))\n",
    "        ll = label_loss(label_pre, torch.tensor(label_tr[i:i+B]).float().to(device))\n",
    "        loss = L*(l0 + l1 + l2 + l3 + l4 + l5) + ll\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    acc_cat = 0\n",
    "    acc_label = 0\n",
    "        \n",
    "    for i in range(0, 500, B):\n",
    "        encoded = tokenizer(tweet_vad[i:i+B], return_tensors='pt', padding= True)\n",
    "        input_ids = encoded['input_ids'].to(device)\n",
    "        attention_mask = encoded['attention_mask'].to(device)\n",
    "        token_type_ids = encoded['token_type_ids'].to(device)\n",
    "        cat_pre,label_pre = model(input_ids, attention_mask, token_type_ids)\n",
    "        for j in range(B):\n",
    "            if(cat_vad[i+j] == torch.argmax(cat_pre[j])):\n",
    "                acc_cat += 1/500\n",
    "            if(label_vad[i+j] == torch.round(label_pre)[j]):\n",
    "                acc_label += 1/500\n",
    "    print(e)\n",
    "    print(\"cat acc during vad:\", acc_cat)\n",
    "    print(\"label acc during vad:\", acc_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a15dadbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # test the performance on the testing dataset\n",
    "    \n",
    "    acc_cat = 0\n",
    "    acc_label = 0\n",
    "        \n",
    "    for i in range(0, len(tweet_te)):\n",
    "        encoded = tokenizer(tweet_te[i], return_tensors='pt', padding= True)\n",
    "        input_ids = encoded['input_ids'].to(device)\n",
    "        attention_mask = encoded['attention_mask'].to(device)\n",
    "        token_type_ids = encoded['token_type_ids'].to(device)\n",
    "        cat_pre,label_pre = model(input_ids, attention_mask, token_type_ids)\n",
    "        if(cat_te[i] == torch.argmax(cat_pre)):\n",
    "            acc_cat += 1/len(tweet_te)\n",
    "        if(label_te[i] == torch.round(label_pre)):\n",
    "            acc_label += 1/len(tweet_te)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c78b5a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat acc during testing: 0.8009596587879542\n",
      "label acc during testing: 0.9082992713701423\n"
     ]
    }
   ],
   "source": [
    "print(\"cat acc during testing:\", acc_cat)\n",
    "print(\"label acc during testing:\", acc_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07216986",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "FP = 0\n",
    "mertics = np.zeros((6, 3)) #tp, fp, fn\n",
    "\n",
    "for i in range(0, len(tweet_te)):\n",
    "        encoded = tokenizer(tweet_te[i], return_tensors='pt', padding= True)\n",
    "        input_ids = encoded['input_ids'].to(device)\n",
    "        attention_mask = encoded['attention_mask'].to(device)\n",
    "        token_type_ids = encoded['token_type_ids'].to(device)\n",
    "        cat_pre,label_pre = model(input_ids, attention_mask, token_type_ids)\n",
    "        if(cat_te[i] == torch.argmax(cat_pre)):\n",
    "            mertics[int(cat_te[i])][0] += 1\n",
    "        else:\n",
    "            mertics[int(cat_te[i])][1] += 1\n",
    "            mertics[int(torch.argmax(cat_pre).item())][2] += 1\n",
    "            \n",
    "        if(label_te[i] == torch.round(label_pre) and label_te[i] > 0):\n",
    "            TP += 1\n",
    "        elif(label_te[i] == torch.round(label_pre) and label_te[i] == 0):\n",
    "            TN += 1\n",
    "        elif(label_te[i] != torch.round(label_pre) and label_te[i] > 0):\n",
    "            FN += 1\n",
    "        else:\n",
    "            FP += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ee45bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label of relevance\n",
      "0.9210169491525424\n",
      "0.9016094242575079\n",
      "floods\n",
      "0.8263221153846154\n",
      "0.8845287873914442\n",
      "explosion\n",
      "0.7223935842072795\n",
      "0.7564599483204134\n",
      "earthquake\n",
      "0.9808350444900753\n",
      "0.9965229485396384\n",
      "hurricane\n",
      "0.7978468899521531\n",
      "0.7065677966101694\n",
      "tornado\n",
      "0.7548711502199874\n",
      "0.6450053705692803\n",
      "bombing\n",
      "0.3504833177424384\n",
      "0.3703459637561779\n"
     ]
    }
   ],
   "source": [
    "label_precision = TP / (TP + FP)\n",
    "label_recall = TP / (TP + FN)\n",
    "cat_0_precision = mertics[0][0] /(mertics[0][0] + mertics[0][1])\n",
    "cat_0_recall = mertics[0][0] /(mertics[0][0] + mertics[0][2])\n",
    "cat_1_precision = mertics[1][0] /(mertics[1][0] + mertics[1][1])\n",
    "cat_1_recall = mertics[1][0] /(mertics[1][0] + mertics[1][2])\n",
    "cat_2_precision = mertics[2][0] /(mertics[2][0] + mertics[2][1])\n",
    "cat_2_recall = mertics[2][0] /(mertics[2][0] + mertics[2][2])\n",
    "cat_3_precision = mertics[3][0] /(mertics[3][0] + mertics[3][1])\n",
    "cat_3_recall = mertics[3][0] /(mertics[3][0] + mertics[3][2])\n",
    "cat_4_precision = mertics[4][0] /(mertics[4][0] + mertics[4][1])\n",
    "cat_4_recall = mertics[4][0] /(mertics[4][0] + mertics[4][2])\n",
    "cat_5_precision = mertics[5][0] /(mertics[0][0] + mertics[5][1])\n",
    "cat_5_recall = mertics[5][0] /(mertics[0][0] + mertics[5][2])\n",
    "\n",
    "print('label of relevance')\n",
    "print(label_precision)\n",
    "print(label_recall)\n",
    "print('floods')\n",
    "print(cat_0_precision)\n",
    "print(cat_0_recall)\n",
    "print('explosion')\n",
    "print(cat_1_precision)\n",
    "print(cat_1_recall)\n",
    "print('earthquake')\n",
    "print(cat_2_precision)\n",
    "print(cat_2_recall)\n",
    "print('hurricane')\n",
    "print(cat_3_precision)\n",
    "print(cat_3_recall)\n",
    "print('tornado')\n",
    "print(cat_4_precision)\n",
    "print(cat_4_recall)\n",
    "print('bombing')\n",
    "print(cat_5_precision)\n",
    "print(cat_5_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c18b2477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show on GeoMap\n",
    "\n",
    "def ShowGeoMap(loc, tweet):\n",
    "    \n",
    "    encoded = tokenizer(tweet.lower(), return_tensors='pt', padding= True)\n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "    token_type_ids = encoded['token_type_ids'].to(device)\n",
    "    cat_pre,label_pre = model(input_ids, attention_mask, token_type_ids)\n",
    "    cat_pred = torch.argmax(cat_pre)\n",
    "    label_pred = torch.round(label_pre)\n",
    "    if (label_pred > 0):\n",
    "        print('Alert, it might be a diseaster')\n",
    "    else:\n",
    "        print('It might not be a diseaster')\n",
    "    print('Probability of related to true diseasters:', str(label_pre.item()*100) +' %')\n",
    "    print('Diseaster Type:', l[cat_pred])\n",
    "    locator = Nominatim(user_agent='myGeocoder')\n",
    "    location = locator.geocode(loc)\n",
    "    center = (location.latitude, location.longitude)\n",
    "    m = Map(center=center, zoom=5)\n",
    "    marker = Marker(location=center, draggable=True)\n",
    "    m.add_layer(marker);\n",
    "    display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e61b57cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# news on diseasters for demo\n",
    "p0 = 'Uttarakhand'\n",
    "t0 = 'The 2021 Uttarakhand flood began on 7 February 2021 in the environs of the Nanda Devi National Park, a UNESCO World Heritage Site[1] in the outer Garhwal Himalayas in Uttarakhand state'\n",
    "p1 = 'Starved Rock State Park'\n",
    "t1 = '3 Dead Following Explosion Near Starved Rock State Park'\n",
    "p2 = 'Truckee'\n",
    "t2 = 'A magnitude 4.7 quake, the largest of the three, occurred at 9:35 p.m. PT about 12 miles northwest of Truckee, California'\n",
    "p3 = 'Florida'\n",
    "t3 = 'DeSantis: $111M infrastructure awards for communities impacted by Hurricane Michael'\n",
    "p4 = 'Texas'\n",
    "t4 = 'Report: Tornado frequency dropping in Texas as ‘tornado alley’ shifts'\n",
    "p5 = 'Boston'\n",
    "t5 = 'After twin blasts shook Boston – killing three and wounding more than 260 others – investigators sprung into action looking for those responsible.'\n",
    "p6 = 'san francisco'\n",
    "t6 = 'Thousands of Californians received notifications for an earthquake they didnt feel. What happened?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "85436531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It might not be a diseaster\n",
      "Probability of related to true diseasters: 0.6484932266175747 %\n",
      "Diseaster Type: explosion\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2b3c54f19e4e8cbb9babf7b22678a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[42.2625621, -71.8018877], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_titl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "place = \n",
    "input_tweet = \n",
    "ShowGeoMap(place, input_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "22edf5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# news on diseasters for demo\n",
    "p0 = 'Uttarakhand'\n",
    "t0 = 'The 2021 Uttarakhand flood began on 7 February 2021 in the environs of the Nanda Devi National Park, a UNESCO World Heritage Site[1] in the outer Garhwal Himalayas in Uttarakhand state'\n",
    "p1 = 'Starved Rock State Park'\n",
    "t1 = '3 Dead Following Explosion Near Starved Rock State Park'\n",
    "p2 = 'Truckee'\n",
    "t2 = 'A magnitude 4.7 quake, the largest of the three, occurred at 9:35 p.m. PT about 12 miles northwest of Truckee, California'\n",
    "p3 = 'Florida'\n",
    "t3 = 'DeSantis: $111M infrastructure awards for communities impacted by Hurricane Michael'\n",
    "p4 = 'Texas'\n",
    "t4 = 'Report: Tornado frequency dropping in Texas as ‘tornado alley’ shifts'\n",
    "p5 = 'Boston'\n",
    "t5 = 'After twin blasts shook Boston – killing three and wounding more than 260 others – investigators sprung into action looking for those responsible.'\n",
    "p6 = 'san francisco'\n",
    "t6 = 'Thousands of Californians received notifications for an earthquake they didnt feel. What happened?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9b7fe595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It might not be a diseaster\n",
      "Probability of related to true diseasters: 0.3091950202360749 %\n",
      "Diseaster Type: explosion\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02994fabac264821995718270e5b8ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[42.2625621, -71.8018877], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_titl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "place = 'worcester'\n",
    "input_tweet = 'I feel happy and great today!!!Yeah!!!'\n",
    "ShowGeoMap(place, input_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66fdb7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
